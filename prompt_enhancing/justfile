set dotenv-load

DEFAULT_INPUT_DIR := "./inputs"
DEFAULT_EXP_NAME := "main"

default:
  @just --list

[group("install")]
[group("notebook")]
install-notebookenv:
  poetry install --all-extras
  poetry run python -m ipykernel install --user --name="mg-train-kernel" --display-name "Python (Magoh training)"
  poetry run nbstripout --install

[group("notebook")]
run-notebooks:
  jupyter notebook notebooks/

[group("notebook")]
run-lab:
  jupyter lab notebooks/

run_main EXP_NAME=DEFAULT_EXP_NAME INPUT_DIR=DEFAULT_INPUT_DIR:
  poetry run main --experiment-name {{EXP_NAME}} --report-dir {{INPUT_DIR}}

try_pipeline:
  poetry run python ./src/try_dataload.py

test:
  poetry run python tests/magoh_transform.py

trace:
  poetry run mlflow ui --port 5000

type_check:
  poetry run mypy .


# TODO: move the ollama server setup in another directory

start-ollama:
  ssh -f $REMOTE_SERVER_SSH_LOGIN "tmux new-session -d -s ollama_main '$OLLAMA_ENV_VARS OLLAMA_HOST=127.0.0.1:$REMOTE_OLLAMA_PORT $OLLAMA_PATH_IN_REMOTE_SERVER serve'; tmux new-session -d -s ollama_llm '$OLLAMA_PATH_IN_REMOTE_SERVER run gemma3:27b'"
  @echo "Requested to start the ollama server"

stop-ollama:
  ssh -f $REMOTE_SERVER_SSH_LOGIN "tmux send-keys -t ollama_llm '\/bye' C-m && tmux send-keys -t ollama_main C-c;"
  @echo "Requested to stop the ollama server"

connect_remote_llm:
  @echo "You can now fetch your llm from http://localhost:$LOCAL_LLM_PORT"
  ssh -N $REMOTE_SERVER_SSH_LOGIN -L $LOCAL_LLM_PORT:localhost:$REMOTE_OLLAMA_PORT
