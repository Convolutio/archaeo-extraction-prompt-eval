{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from archaeo_super_prompt.dataset.load import MagohDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MagohDataset(200, 0.8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from try_dataload import pipeline, inputs_from_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "With exploring the layout and the profile of the downloaded reports, we select some intervention identifiers that can be processable for a study of the chunks and the LLM interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "selected_ids = {\n",
    "31049\n",
    "}\n",
    "inputs = { 31049: [Path(\".cache/pdfs/31049/Relazione_storica_Pasquinucci.pdf\").resolve()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pipeline.fit_transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_pdf = texts[31049][\"Relazione_storica_Pasquinucci.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(read_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Chunk information\n",
    "\n",
    "From each chunk, we can have the following information:\n",
    "\n",
    "- a simple type (paragraph, list item, table)\n",
    "- its page number to have its approximate position in the document (beginning, middle, end, ...)\n",
    "- its context text, including :\n",
    "  - the description of the predicted section it belongs to\n",
    "  - the text rendering of the chunk content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "chunks = read_pdf.chunks()\n",
    "SAMPLE_CHUNK_NUMBER = 4\n",
    "sample_chunks = sample(chunks, 4)\n",
    "total_page_number = max(chunk.page_idx for chunk in chunks)\n",
    "\n",
    "TAG_TO_STRING = {\n",
    "\"para\":\"Paragraph\", \"list_item\":\"List item\", \"table\": \"Table\", \"header\": \"Header\"\n",
    "}\n",
    "\n",
    "for chunk in sample_chunks:\n",
    "    print(f\"Page {chunk.page_idx} over {total_page_number} ({TAG_TO_STRING[chunk.tag]})\")\n",
    "    print(\"-\"*60)\n",
    "    print(chunk.to_context_text())\n",
    "    print(\"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Exploit the chunk information for the prompts\n",
    "\n",
    "The contextual text can be compared to the query with an embedding model.  \n",
    "The other information (type of content, position in the document, entity occurences) can also be used to select the best chunks for some extraction queries, according to the nature of the field to be extracted and the information we have about the excavation reports which compose the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
