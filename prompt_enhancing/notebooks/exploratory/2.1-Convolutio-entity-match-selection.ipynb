{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown, Markdown\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from archaeo_super_prompt.dataset import MagohDataset\n",
    "from archaeo_super_prompt.dataset.thesaurus import load_comune\n",
    "from archaeo_super_prompt.modeling.pdf_to_text import VLLM_Preprocessing\n",
    "import archaeo_super_prompt.modeling.entity_extractor.model as ner_module\n",
    "from archaeo_super_prompt.modeling.entity_extractor import NamedEntityField\n",
    "from archaeo_super_prompt.visualization.entities import visualize_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Chunk pre-selection with Named-Entity matching\n",
    "\n",
    "For some fields with a know thesaurus set, the values are directly occuring in the document. Then, to infer the class of those fields, we figure out with Named-Entity-Extraction (NER) model if their thesaurus are present and in which text chunks.\n",
    "\n",
    "At the end of this pre-selection, the following information are providable to data extraction model :\n",
    "- the $k$ more relevant chunks with the presence of some thesaurus of the field\n",
    "- the thesaurus values already identified\n",
    "\n",
    "The LLM data extraction model can then just read the few chunks and output the more likely thesaurus among the provided ones through a simple ChainOfThought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Sample ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES_FETCHED = 300\n",
    "SEED = 0.5\n",
    "\n",
    "dataset = MagohDataset(MAX_SAMPLES_FETCHED, SEED, True)\n",
    "_selected_ids = [35983, 31298]\n",
    "selected_ids = set(_selected_ids)\n",
    "inputs = dataset.get_files_for_batch(selected_ids)\n",
    "\n",
    "\n",
    "def ingest():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    TIMEOUT_PER_PAGE = 60 * 3\n",
    "    # Example using the Granite Vision model with Ollama:\n",
    "    doc_converter = VLLM_Preprocessing(\n",
    "        model=\"granite3.2-vision:latest\",\n",
    "        prompt=\"OCR this part of Italian document for markdown-based processing.\",\n",
    "        embedding_model_hf_id=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "        max_chunk_size=512,\n",
    "        allowed_timeout=TIMEOUT_PER_PAGE,\n",
    "    )\n",
    "    results = doc_converter.transform(inputs)\n",
    "    return results\n",
    "\n",
    "\n",
    "ready_to_be_processed = ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_be_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Named-Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_module functions: fetch_entities, postprocess_entities, filter_entities, extract_wanted_entities\n",
    "\n",
    "batch_entities = ner_module.fetch_entities(\n",
    "    list(\n",
    "        map(lambda row: row.chunk_content, ready_to_be_processed.itertuples())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppss = ner_module.postrocess_entities(batch_entities, 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualized = [\n",
    "    visualize_entities(content, entities)\n",
    "    for content, entities in zip(\n",
    "        list(\n",
    "            map(\n",
    "                lambda row: row.chunk_content,\n",
    "                ready_to_be_processed.itertuples(),\n",
    "            )\n",
    "        ),\n",
    "        ppss,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(visualized):\n",
    "    for vi in visualized:\n",
    "        display_markdown(Markdown(vi))\n",
    "        print(\"--\" * 5)\n",
    "\n",
    "\n",
    "visualize(visualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Filter extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ppss = list(ner_module.filter_entities(ppss, [\"LUOGO\", \"INDIRIZZO\", \"CODICE_POSTALE\"]))\n",
    "\n",
    "f_visualized = [\n",
    "    visualize_entities(content, entities)\n",
    "    for content, entities in zip(\n",
    "        list(\n",
    "            map(\n",
    "                lambda row: row.chunk_content,\n",
    "                ready_to_be_processed.itertuples(),\n",
    "            )\n",
    "        ),\n",
    "        filtered_ppss,\n",
    "    )\n",
    "]\n",
    "\n",
    "visualize(f_visualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Filter chunks according to occured entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "comune_field_to_be_figured_out = NamedEntityField(\n",
    "    \"comune\",\n",
    "    {\"INDIRIZZO\", \"CODICE_POSTALE\", \"LUOGO\"},\n",
    "    load_comune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_comune = ner_module.extract_wanted_entities(\n",
    "    list(\n",
    "            map(\n",
    "                lambda row: row.chunk_content,\n",
    "                ready_to_be_processed.itertuples(),\n",
    "            )\n",
    "        ),\n",
    "    ner_module.filter_entities(\n",
    "        ppss, comune_field_to_be_figured_out.compatible_entities\n",
    "    ),\n",
    "    comune_field_to_be_figured_out.thesaurus_values,\n",
    "    0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entities_matches in with_comune:\n",
    "    print(entities_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
