{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from docling.datamodel.pipeline_options_vlm_model import ResponseFormat\n",
    "from pathlib import Path\n",
    "\n",
    "from archaeo_super_prompt.dataset.load import MagohDataset\n",
    "from archaeo_super_prompt.pdf_to_text.stream_ocr import process_documents, converter, ollama_vlm_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ðŸ¦†ðŸ“ƒ PDF complete ingestion with Docling preprocessing\n",
    "\n",
    "We try the young framework Docling and its usage of VLLM on the HuggingFace repositories to achieve thoses tasks:\n",
    "\n",
    "- document OCR with Italian language analysis (VLLM)\n",
    "- document chunking with these features:\n",
    "    - layout-aware\n",
    "    - smart tokenization\n",
    "\n",
    "All of these things are possible with incorporating several open ML models into the Docling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES_FETCHED = 300\n",
    "SEED = 0.5\n",
    "\n",
    "dataset = MagohDataset(MAX_SAMPLES_FETCHED, SEED, True)\n",
    "_selected_ids = [\n",
    "    35983, 31298\n",
    "]\n",
    "selected_ids = set(_selected_ids)\n",
    "inputs = dataset.get_files_for_batch(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"filepath\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    TIMEOUT_PER_PAGE = 60*3\n",
    "    # Example using the Granite Vision model with Ollama:\n",
    "    vlm_options = ollama_vlm_options(\n",
    "        model=\"granite3.2-vision:latest\",\n",
    "        prompt=\"OCR the full page for markdown-based processing.\",\n",
    "        # Doctags is only supported by doclings vllm for now\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "        allowed_timeout=TIMEOUT_PER_PAGE\n",
    "    )\n",
    "    doc_converter = converter(vlm_options)\n",
    "    # results = doc_converter.convert(inputs[\"filepath\"].tolist()[1])\n",
    "    results = process_documents([Path(p) for p in inputs[\"filepath\"].tolist()],\n",
    "                                doc_converter, TIMEOUT_PER_PAGE)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Inspect the results\n",
    "\n",
    "We export into markdown the results for display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(result[0].document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import ConversionStatus\n",
    "\n",
    "\n",
    "def is_successful(conv_res):\n",
    "    return (conv_res.status == ConversionStatus.SUCCESS or\n",
    "            conv_res.status == ConversionStatus.PARTIAL_SUCCESS)\n",
    "\n",
    "\n",
    "def filename(conv_res):\n",
    "    return conv_res.input.file.stem\n",
    "\n",
    "[{\"good\": is_successful(r), \"name\": filename(r)} for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HierarchicalChunker\n",
    "\n",
    "chunker = HierarchicalChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=result[1].document)\n",
    "chunks = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== {i} ===\")\n",
    "    txt_tokens = len(chunk.text.rstrip().split(\" \"))  # tokenizer.count_tokens(chunk.text)\n",
    "    print(f\"chunk.text ({txt_tokens} tokens):\\n{chunk.text!r}\")\n",
    "\n",
    "    ser_txt = chunker.contextualize(chunk=chunk)\n",
    "    ser_tokens = len(ser_txt.rstrip().split(\" \"))  # tokenizer.count_tokens(ser_txt)\n",
    "    print(f\"chunker.contextualize(chunk) ({ser_tokens} tokens):\\n{ser_txt!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(result[1].document.export_to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
